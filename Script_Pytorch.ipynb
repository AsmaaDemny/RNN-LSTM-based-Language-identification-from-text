{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_PTyli1_zbd"
      },
      "source": [
        "# ***Import libraries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DpeVZkWdnOc"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import fnmatch\n",
        "from IPython.display import Audio\n",
        "from IPython.display import display\n",
        "from matplotlib import pyplot as pl\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import nltk \n",
        "import glob\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5itTb0Ug_4MM"
      },
      "source": [
        "# ***Select the Cuda Device***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w08-w-ByYP8",
        "outputId": "39ae8816-a912-4e92-f14a-b73116804b13"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # using cuda device to procss the data fastly  \n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akPKoOH1ABZs"
      },
      "source": [
        "# ***Read the languges Files***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzsGiCH9W3rG"
      },
      "source": [
        "txt_files_list = glob.glob(\"language_data/*.txt\") # folder that contain all txt  files  \n",
        "language = 0\n",
        "df = pd.DataFrame(columns=['sentences','languages']) # create dataframe that hold all data\n",
        "sentences = [] # create the list that hold all sentences in all documents \n",
        "languages = [] # create the list that hold the languages with the same index of sentnce \n",
        "for filename in txt_files_list:\n",
        "    with open(filename, 'r' , errors=\"ignore\") as f:\n",
        "        Lines = f.readlines()\n",
        "        for line in Lines:\n",
        "            line = line.replace(\"\\n\",\"\") # clean sentence from document next line symbole\n",
        "            line = re.sub(r'[0-9]+', '', line) # clean sentence from numbers\n",
        "            line = re.sub('[!@#$ª•ย™˚\\\\±“‟%„<{―=½≈»_]', '', line) # clean sentence from specific symbols \n",
        "            if len(line) > 30 : \n",
        "              sentences.append(line[30]) # add the  sentence to sentences list \n",
        "            else : \n",
        "              sentences.append(line) # add the  sentence to sentences list \n",
        "            languages.append(language) # add the  languages to languages list \n",
        "        language += 1\n",
        "df['sentences'] = sentences # add sentence to the dataframe\n",
        "df['languages'] = languages  # add languages to the dataframe for each sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-JcS1TeXlDp"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "df = shuffle(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPgV-__bumiz"
      },
      "source": [
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\" + u'\\xab'\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "def unicodeToAscii(s): # # Turn a Unicode string to plain ASCII, \n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz8j9vXlu5nL"
      },
      "source": [
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HllKDRKq5-aE"
      },
      "source": [
        "all_languages = {0:'Catalan', 1 : 'Danish' , 3 : 'Dutch' , 4: 'French' , 5: 'German' , 6: 'Italian' ,7: 'Portuguese' ,8: 'swedish'} # dictionary for all langauges  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHFnBIHfFtYn"
      },
      "source": [
        "# ***The dataset Class***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MIR1uB0ddnOl"
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self,data):\n",
        "        self.data = data\n",
        "        self.sentences = data.iloc[:,:-1].values # take all the sentences in the dataframe for train or test data \n",
        "        self.label = data.iloc[:,-1].values # take all the langauges in the dataframe for train or test data \n",
        "\n",
        "        #self.sentence = sorted(self._find_files(sentences))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len (self.sentences)\n",
        "\n",
        "\n",
        "#in getitem method ,i convert the sentence into list of tensor values\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        sentence = self.sentences [index]\n",
        "        sentence = sentence[0]\n",
        "        features = lineToTensor(sentence) # function to convert the sentence into one-hot encoding \n",
        "        label = [self.label [index]] # No changing for the languages tensor \n",
        "        return features , torch.tensor(label)\n",
        "    \n",
        "    def tensor2char(self,index): # function to decode the tensor \n",
        "        sentence , label = self.__getitem__(index)\n",
        "        sentence= sentence.numpy()\n",
        "        text = [idx2char[idx] for idx in sentence]\n",
        "        return \"\".join(text) , label\n",
        "    \n",
        "    def _find_files(self, directory, pattern='*.txt'):\n",
        "        \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
        "        files = []\n",
        "        for root, dirnames, filenames in os.walk(directory):\n",
        "            for filename in fnmatch.filter(filenames, pattern):\n",
        "                files.append(os.path.join(root, filename))\n",
        "        return files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXdFYbUyFw2l"
      },
      "source": [
        "# ***The Collate_fn class***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "VhXWZvnldnOn"
      },
      "source": [
        "class TextCollate(object):\n",
        "    \"\"\"Function object used as a collate function for DataLoader.\"\"\"\n",
        "\n",
        "    def __init__(self,):\n",
        "        \n",
        "        pass\n",
        "        \n",
        "    def _collate_fn(self, batch):\n",
        "          (xx, yy) = zip(*batch)\n",
        "          #x_lens = [len(x) for x in xx]\n",
        "          #xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
        "          #yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)  \n",
        "          return xx , yy # in each batch it return the sentences with their labels\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self._collate_fn(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY4PmTgpW4g7"
      },
      "source": [
        "# ***Get DATA FILES and create dataframe***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "RAmVRbsRc793",
        "outputId": "5dde4860-cf1a-415c-a931-c5f15369c400"
      },
      "source": [
        "MAX_LENGTH = df.sentences.str.len().max()\n",
        "print(MAX_LENGTH) # the maximum length in sentences\n",
        "df.groupby(['languages']).count() # check all the languages in the dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>languages</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>493026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>128916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>471902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>501685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>121479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>89358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>250967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>433318</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           sentences\n",
              "languages           \n",
              "0             493026\n",
              "1             128916\n",
              "2             471902\n",
              "3             501685\n",
              "4             121479\n",
              "5              89358\n",
              "6             250967\n",
              "7             433318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0cAf4-riMEm"
      },
      "source": [
        "**Split the data into train and test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rx23ka6o8Sv"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size=0.2) # split the data to 80% for train and 20% for test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vT4zmOpjpoV"
      },
      "source": [
        "***Create loader***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "veL3faSodnOo"
      },
      "source": [
        "pad_collate = TextCollate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZOq3cgUjjyY"
      },
      "source": [
        "BATCH_SIZE = 32 # number of sentences in each batch\n",
        "num_workers = 4 # number of workers to upload the batch to the RAM\n",
        "dataset_train = TextDataset(data= train)\n",
        "train_iterator =  DataLoader(dataset_train, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=pad_collate, num_workers=num_workers)\n",
        "\n",
        "dataset_test = TextDataset(data= test)\n",
        "test_iterator = DataLoader(dataset_test, batch_size=BATCH_SIZE, collate_fn= pad_collate, shuffle=True, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPO_wS1sxZpp",
        "outputId": "c769fe9e-6ef3-4895-e0d2-6676b500ccbb"
      },
      "source": [
        "print(len(train_iterator) , len(test_iterator)) # explore the number of iteration needed to pass throw all batches\n",
        "# train_len / batch_size "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1563 1563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ5JUJHav04E"
      },
      "source": [
        "dataiter = iter(train_iterator) # iterate oven one batch just to see the structure\n",
        "data = dataiter.next()\n",
        "sentences , label= data # each batch contain number of sentences with their suitable languages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpw6SQzd2R8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1980cfc-0a7e-4c3f-dde4-5576c773b6f2"
      },
      "source": [
        "print(sentences[0]) # print the first sentence in the batch "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JZ3Ji1f6-HF"
      },
      "source": [
        "***Convert the Output probability to Language name ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju0T3CKA520z"
      },
      "source": [
        "def categoryFromOutput(output):\n",
        "  language_idx = torch.argmax(output).item() # pick the index of the maximum value\n",
        "  return language_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39rU_qG7EqD"
      },
      "source": [
        "# ***Create the RNN Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b8Bx3aWXuVd"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size) # the first hidden layer\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size) # the output  layer\n",
        "        self.softmax = nn.LogSoftmax(dim=1) # use softmaxe function to calculate the probability over each class\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5nXI_kNq2RP"
      },
      "source": [
        "# ***Hyper-parameters***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ApNvxMSqvsS"
      },
      "source": [
        "num_epochs = 3 # number of epochs\n",
        "n_hidden = 256 # number of units\n",
        "n_categories = len(all_languages) # number of classes\n",
        "criterion = nn.NLLLoss() # the Loss function because its a classification problem with 8 classes.\n",
        "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
        "Model = RNN(n_letters, n_hidden, n_categories).to(device) # initilize the model \n",
        "optimizer = torch.optim.SGD(Model.parameters(), lr=learning_rate) # Stochastic gradient descent "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBv61E3prCzx"
      },
      "source": [
        "***Function that return some random examples from the dataframe***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSkKnBUhbd-f"
      },
      "source": [
        "def randomTrainingExample(iter):\n",
        "    \n",
        "    line = df['sentences'].values[iter]\n",
        "    cat = df['languages'].values[iter]\n",
        "    category_tensor = torch.tensor([cat], dtype=torch.long)\n",
        "    line_tensor = lineToTensor(line)\n",
        "    return  category_tensor, line_tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sck-6WlxMPN"
      },
      "source": [
        "# ***Create Training Lopp***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hscBh3HWUjR"
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AEfwy-VajsS"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "n_iters = 10000\n",
        "print_every = 1000\n",
        "plot_every = 1000\n",
        "\n",
        "\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "loss =  0\n",
        "num_epochs = 20\n",
        "total_step = len(train_iterator)\n",
        "hidden = Model.initHidden()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (sentences, labels) in enumerate(train_iterator):\n",
        "\n",
        "        for j , sent in enumerate(sentences):\n",
        "\n",
        "            hidden = Model.initHidden()  # create the initial hidden tensor\n",
        "            hidden = hidden.to(device)  # use cuda device to the hidden tensor\n",
        "            sent = sent.to(device) # use cuda device to the sentence tensor\n",
        "            for k in range(sent.size()[0]):\n",
        "                output, hidden = Model(sent[k], hidden)\n",
        "\n",
        "            loss = criterion(output, labels[j].to(device)) # calculate the loss between the predict and target\n",
        "            \n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad() # turn the gradient to zero\n",
        "            loss.backward() # calculate the derivative of the loss over the model\n",
        "            torch.nn.utils.clip_grad_norm_(Model.parameters(), 0.25) # using the gradient clipper to avoid exploiding\n",
        "            optimizer.step() # Apply the parameters update\n",
        "            #hidden = hidden.detach()\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-gRO3F27JwJ"
      },
      "source": [
        "# ***Create the Testing loop***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVMZ3sGfmWGi"
      },
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_step = len(test_iterator)\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (sentences, labels) in enumerate(test_iterator):\n",
        "\n",
        "            for j , sent in enumerate(sentences):\n",
        "\n",
        "                hidden = Model.initHidden()\n",
        "                \n",
        "                hidden = hidden.to(device) \n",
        "                sent = sent.to(device)\n",
        "                for k in range(sent.size()[0]):\n",
        "                    output, hidden = Model(sent[k], hidden)\n",
        "                    \n",
        "                predicted = categoryFromOutput(output) # pick the maximun value on the output tensor\n",
        "                \n",
        "                total += labels[j].size(0)\n",
        "                \n",
        "                correct += (predicted == labels[j]).sum().item() # accumulate the number of correct prediction of the target value equal the output value\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test Sentences: {} %'.format(100 * correct / total)) \n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(Model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}